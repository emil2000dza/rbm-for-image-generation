{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RBM_notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "ZYJ55wL8yFX2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeV36kSZ4BZU"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import math\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import keras\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bases de données"
      ],
      "metadata": {
        "id": "QRvHEX7yyKKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = scipy.io.loadmat('binaryalphadigs.mat') "
      ],
      "metadata": {
        "id": "Fr_N8fbc4Kc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(Xtrain, ytrain), (Xtest, ytest) = mnist.load_data()\n",
        "\n",
        "def digits(digit):\n",
        "  '''\n",
        "  Prend en argument un chiffre digit et retourne les images MNIST 28x28 correspondant\n",
        "  à ce label\n",
        "  '''\n",
        "  digits=[]\n",
        "  for i in range(len(Xtrain)):\n",
        "    if ytrain[i] == digit:\n",
        "      digits.append(Xtrain[i])\n",
        "  return digits"
      ],
      "metadata": {
        "id": "6VK1WkLRYNaf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fonctions auxiliaires"
      ],
      "metadata": {
        "id": "42cNax4CyM7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(X):\n",
        "  '''\n",
        "  Prend en argument un vecteur X et retourne la fonction sigmoid associée à ce vecteur\n",
        "  '''\n",
        "  return 1 / (1 + np.exp(-X))\n",
        "\n",
        "def softmax(X,w,b):\n",
        "  '''\n",
        "  Prend en argument un vecteur X et retourne la fonction softmax associée à ce vecteur\n",
        "  '''\n",
        "  z=np.dot(X,w)+b\n",
        "  return np.exp(z)/np.transpose([np.sum(np.exp(z),axis=1)])\n",
        "\n",
        "def mini_batches(X,batch_size):\n",
        "  '''\n",
        "  Prend en argument une liste X et la sépare en une liste de batch de tailles resepectives batch_size \n",
        "  '''\n",
        "  data_size=X.shape[0]\n",
        "  batch = [] \n",
        "  nb_batch = data_size // batch_size\n",
        "  reste=data_size % batch_size\n",
        "  \n",
        "  for i in range(nb_batch): \n",
        "    mini_batch = X[i * batch_size:(i + 1)*batch_size] \n",
        "    batch.append([x.flatten() for x in mini_batch])\n",
        "  if reste != 0:\n",
        "    mini_batch=X[data_size-reste:data_size]\n",
        "    mini_batch=np.append(mini_batch,X[0:batch_size-reste],axis=0)\n",
        "    batch.append([x.flatten() for x in mini_batch])\n",
        "  return batch"
      ],
      "metadata": {
        "id": "790_zP8t58eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction d'un RBM et test sur AlphaDigit"
      ],
      "metadata": {
        "id": "k8xwywFdyQmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lire_alpha_digit(indice_caractere,num):\n",
        "  '''\n",
        "  Renvoie les images liées à un nombre d'indice de caractère indice_caractere\n",
        "  '''\n",
        "  nombre=alpha[\"dat\"][indice_caractere,num]\n",
        "  return nombre\n",
        "\n",
        "class RBM():\n",
        "  def __init__(self,n_v, n_h):\n",
        "    self.a=[0 for i in range(n_v)]\n",
        "    self.b=[0 for i in range(n_h)]\n",
        "    self.w=np.random.normal(0,0.1,(n_v,n_h))\n",
        "\n",
        "  def entree_sortie_RBM(self,V):\n",
        "     sig=sigmoid(np.dot(V,self.w)+self.b)\n",
        "     Hs=np.random.binomial(1, sig, size=sig.shape)\n",
        "     return np.array(sig),Hs\n",
        "     \n",
        "  def sortie_entree_RBM(self,H):\n",
        "     q=sigmoid(np.dot(H,np.transpose(self.w))+self.a)\n",
        "     Vs=np.random.binomial(1, q, size=q.shape)\n",
        "     return np.array(q),Vs\n",
        "  \n",
        "  def train_RBM(self,X,batch_size,epochs,learning_rate):\n",
        "    for j in range(epochs):\n",
        "      batches=mini_batches(X,batch_size)\n",
        "      loss=0\n",
        "      somme=0\n",
        "      for i in batches:\n",
        "        V=np.array(i)\n",
        "        p,Hs=RBM.entree_sortie_RBM(self,V)\n",
        "        q1,V1= RBM.sortie_entree_RBM(self,Hs)\n",
        "        p1,H1= RBM.entree_sortie_RBM(self,V1)\n",
        "        dw= (1/batch_size)*(np.dot(np.transpose(V),p)-np.dot(np.transpose(V1),p1))\n",
        "        da= (1/batch_size)* np.sum(V-V1,axis=0)\n",
        "        db= (1/batch_size)* np.sum(p-p1,axis=0)\n",
        "        self.w+=learning_rate*dw\n",
        "        self.a+=learning_rate*da\n",
        "        self.b+=learning_rate*db\n",
        "        loss+=np.mean(np.square(V1-V))\n",
        "        somme+=1\n",
        "      print(\"Epoch \" + str(j)+ \" Quadratic loss = \" + str(loss/somme))\n",
        "    return str(loss/somme)"
      ],
      "metadata": {
        "id": "0Egtln4s4VOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generer_image_RBM(X,n_images,rbm,batch_size,epochs,learning_rate,iter_gibbs):\n",
        "  rbm.train_RBM(X,batch_size,epochs,learning_rate)\n",
        "  L,l=np.shape(X[0])\n",
        "  for i in range(n_images):\n",
        "    v=np.random.randint(2,size=L*l)\n",
        "    for k in range(iter_gibbs):\n",
        "      ph,h=rbm.entree_sortie_RBM(v)\n",
        "      pv,v=rbm.sortie_entree_RBM(h)\n",
        "\n",
        "      \n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(L,l))\n",
        "    plt.subplot(221)\n",
        "    plt.imshow(np.array(X[i]).reshape(L,l))\n",
        "\n",
        "    plt.subplot(222)\n",
        "    plt.imshow(np.array(v).reshape((L,l)))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lTXxck_YCac8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparaison entre les vraies images et les images générées par le RBM"
      ],
      "metadata": {
        "id": "rgZQb98byZDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resultats_RBM(n_images,indices_caractere,arc,batch_size,epochs,learning_rate,iter_gibbs):\n",
        "  '''\n",
        "  Prend en argument le nombre d'images à générer, l'indice du caractère que l'on veut génerer, \n",
        "  le nombre d'unités cachées, la taille du batch,\n",
        "  le nombre d'epochs, le learning rate et le nombre d'itérations de Gibbs\n",
        "  Renvoie les images générées ainsi que des images de la base de données et les résultats de l'entraînement\n",
        "  '''\n",
        "  X=np.concatenate( [alpha[\"dat\"][i] for i in [indices_caractere]])\n",
        "  #np.random.shuffle(X)\n",
        "  L,l=np.shape(X[0])\n",
        "  rbm=RBM(L*l,arc)\n",
        "  final_loss=rbm.train_RBM(X,batch_size,epochs,learning_rate)\n",
        "  generer_image_RBM(X,n_images,rbm,batch_size,epochs,learning_rate,iter_gibbs)\n",
        "\n",
        "resultats_RBM(10,9,100,6,100,0.1,100)"
      ],
      "metadata": {
        "id": "-aGeU5UK46Q3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimisation selon le learning rate et le nombre d'unités arc\n",
        "On cherche ici à trouver les hyperparamètres optimales pour le RBM appliqué à AlphaDigit. "
      ],
      "metadata": {
        "id": "hJTpG4UvAep5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimisation_lr(n_images,indices_caractere,arc,batch_size,epochs,learning_rates,iter_gibbs):\n",
        "  final_loss=[]\n",
        "  for i in range(len(learning_rates)):\n",
        "    X=np.concatenate( [alpha[\"dat\"][i] for i in [indices_caractere]])\n",
        "    L,l=np.shape(X[0])\n",
        "    rbm=RBM(L*l,arc)\n",
        "    final_loss.append(rbm.train_RBM(X,batch_size,epochs,learning_rates[i]))\n",
        "  return final_loss\n",
        "\n",
        "def optimisation_units(n_images,indices_caractere,arc,batch_size,epochs,learning_rate,iter_gibbs):\n",
        "  final_loss=[]\n",
        "  for i in range(len(arc)):\n",
        "    X=np.concatenate( [alpha[\"dat\"][i] for i in [indices_caractere]])\n",
        "    L,l=np.shape(X[0])\n",
        "    rbm=RBM(L*l,arc[i])\n",
        "    final_loss.append(rbm.train_RBM(X,batch_size,epochs,learning_rate))\n",
        "  print(final_loss)\n",
        "  return final_loss\n",
        "\n",
        "optimisation_units(10,25,[2,5,10,50,100,200,400,600,800,1000,4000],6,100,0.01,10)"
      ],
      "metadata": {
        "id": "_KvPPKuN2j89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On affiche les valeurs de loss pour les différents paramètres testés. "
      ],
      "metadata": {
        "id": "k_a-3e8VAsEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "opti=optimisation_units(10,25,[2,5,10,50,100,200,400,600,800,1000,4000],6,100,0.01,10)\n",
        "plt.plot([2,5,10,50,100,200,400,600,800,1000,4000],[float(opti[i]) for i in range(len(opti))])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q76w6dFH-EtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparaison entre les chiffres de MNIST et les chiffres générés par le RBM\n",
        "On crée une nouvelle classe RBM2 strictement similaire à RBM mais n'utilisant pas de batch. "
      ],
      "metadata": {
        "id": "449SX5mkyf1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM2():\n",
        "  def __init__(self,n_v, n_h):\n",
        "    self.a=[0 for i in range(n_v)]\n",
        "    self.b=[0 for i in range(n_h)]\n",
        "    self.w=np.random.normal(0,0.1,(n_v,n_h))\n",
        "\n",
        "  def entree_sortie_RBM(self,V):\n",
        "     sig=sigmoid(np.dot(V,self.w)+self.b)\n",
        "     Hs=np.random.binomial(1, sig, size=sig.shape)\n",
        "     return np.array(sig),Hs\n",
        "     \n",
        "  def sortie_entree_RBM(self,H):\n",
        "     q=sigmoid(np.dot(H,np.transpose(self.w))+self.a)\n",
        "     Vs=np.random.binomial(1, q, size=q.shape)\n",
        "     return np.array(q),Vs\n",
        "  \n",
        "  def train_RBM(self,X,epochs,learning_rate):\n",
        "    for j in range(epochs):\n",
        "      loss=0\n",
        "      somme=0\n",
        "      Xs=[]\n",
        "      for i in range(len(X)):\n",
        "        Xs.append([ X[i].flatten() ])\n",
        "      X_size=len(X)\n",
        "      for i in Xs:\n",
        "        V=np.array(i)\n",
        "        p,Hs=RBM.entree_sortie_RBM(self,V)\n",
        "        q1,V1= RBM.sortie_entree_RBM(self,Hs)\n",
        "        p1,H1= RBM.entree_sortie_RBM(self,V1)\n",
        "        dw= (1/X_size)*(np.dot(np.transpose(V),p)-np.dot(np.transpose(V1),p1))\n",
        "        da= (1/X_size)* np.sum(V-V1,axis=0)\n",
        "        db= (1/X_size)* np.sum(p-p1,axis=0)\n",
        "        self.w+=learning_rate*dw\n",
        "        self.a+=learning_rate*da\n",
        "        self.b+=learning_rate*db\n",
        "        loss+=np.mean(np.square(V1-V))\n",
        "        somme+=1\n",
        "      print(\"Epoch \" + str(j)+ \" Quadratic loss = \" + str(loss/somme))\n",
        "    return str(loss/somme)"
      ],
      "metadata": {
        "id": "EendL9HFnSoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_MNIST(X):\n",
        "  '''\n",
        "  Prend en argument une image de la base de données MNIST 28x28 \n",
        "  et renvoie cette image en binaire (pixel=0 ou pixel=1)\n",
        "  '''\n",
        "  Y=[[0 for i in range(len(X))]for j in range(len(X[0]))]\n",
        "  for i in range(len(X)):\n",
        "    for j in range(len(X[0])):\n",
        "      if X[i][j] !=0:\n",
        "        Y[i][j]=1\n",
        "      else:\n",
        "        Y[i][j]=0\n",
        "  npy=np.array(Y)\n",
        "  #plt.figure()\n",
        "  #plt.imshow(npy.reshape(len(Y),len(Y[i])))\n",
        "  return Y\n",
        "\n",
        "def generer_image_RBM2(X,n_images,rbm,epochs,learning_rate,iter_gibbs):\n",
        "  rbm.train_RBM(X,epochs,learning_rate)\n",
        "  L,l=np.shape(X[0])\n",
        "  for i in range(n_images):\n",
        "    v=np.random.randint(2,size=L*l)\n",
        "    for k in range(iter_gibbs):\n",
        "      ph,h=rbm.entree_sortie_RBM(v)\n",
        "      pv,v=rbm.sortie_entree_RBM(h)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(L,l))\n",
        "    plt.subplot(221)\n",
        "    plt.imshow(np.array(X[i]).reshape(L,l))\n",
        "\n",
        "    plt.subplot(222)\n",
        "    plt.imshow(np.array(v).reshape((L,l)))\n",
        "    plt.show()\n",
        "\n",
        "def resultats_RBM_for_MNIST(n_images,digit,n_entrainement,arc,epochs,learning_rate,iter_gibbs):\n",
        "  '''\n",
        "  Prend en argument le nombre d'images à générer, le chiffre que l'on veut génerer, \n",
        "  le nombre de données d'entraînement, le nombre d'unités cachées,\n",
        "  le nombre d'epochs, le learning rate et le nombre d'itérations de Gibbs\n",
        "  Renvoie les images générées comparées à des images de la base de données d'entraînement et les résultats de l'entraînement\n",
        "  '''\n",
        "  X=np.concatenate([ [pre_process_MNIST(digits(digit)[i])] for i in range(n_entrainement+1)])\n",
        "  L,l=np.shape(X[0])\n",
        "  rbm=RBM2(L*l,arc)\n",
        "  final_loss=rbm.train_RBM(X,epochs,learning_rate)\n",
        "  generer_image_RBM2(X,n_images,rbm,epochs,learning_rate,iter_gibbs)\n",
        "\n",
        "resultats_RBM_for_MNIST(10,3,2000,800,100,0.1,100)"
      ],
      "metadata": {
        "id": "ARfF9J7tYGBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Génération de chiffres MNIST par GAN\n",
        "On utilise maintenant une architecture GAN pour générer les chiffres de la base de données MNIST. "
      ],
      "metadata": {
        "id": "li-akVFDypTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Base de données"
      ],
      "metadata": {
        "id": "Mci8a4iX06K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 64\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           num_workers=num_workers)"
      ],
      "metadata": {
        "id": "-jO9ErHByDgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (3,3)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "id": "VUbLOnaFyxUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture du disciriminator et du generator"
      ],
      "metadata": {
        "id": "sK1yJCYn0-58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_dim, output_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
        "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
        "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, output_size)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # flatten image\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc4(x)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Erix7yxPy-4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_dim, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
        "        self.fc4 = nn.Linear(hidden_dim*4, output_size)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        out = F.tanh(self.fc4(x))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "3785pkk3zMRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Définition des classes"
      ],
      "metadata": {
        "id": "JGVwan5o1E-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator\n",
        "input_size = 784 #taille de l'image d'entrée\n",
        "d_output_size = 1 #taille du discriminateur en sortie\n",
        "d_hidden_size = 32 # taille de la dernière couches\n",
        "\n",
        "\n",
        "# Generator\n",
        "z_size = 100 # taille du vecteur latent\n",
        "g_output_size = 784 # taille du discriminator en sortie\n",
        "g_hidden_size = 32 # taille de la première couche cachée"
      ],
      "metadata": {
        "id": "lS6Elx8QzUgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
        "G = Generator(z_size, g_hidden_size, g_output_size)"
      ],
      "metadata": {
        "id": "hvTPkx9Ez3NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Définition des fonctions de pertes"
      ],
      "metadata": {
        "id": "eHRTuvRm1IZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def real_loss(D_out, smooth=False):\n",
        "    batch_size = D_out.size(0)\n",
        "    if smooth:\n",
        "        labels = torch.ones(batch_size)*0.9\n",
        "    else:\n",
        "        labels = torch.ones(batch_size) # real labels = 1\n",
        "        \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.zeros(batch_size) # fake labels = 0\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "WZ3W1A9Kz8Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.002\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)"
      ],
      "metadata": {
        "id": "T5WdNw_E0DYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entraînement"
      ],
      "metadata": {
        "id": "xKBV6wN61McO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "samples = []\n",
        "losses = []\n",
        "\n",
        "print_every = 400\n",
        "\n",
        "sample_size=16\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\n",
        "\n",
        "D.train()\n",
        "G.train()\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
        "                \n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n",
        "        \n",
        "        d_optimizer.zero_grad()\n",
        "        \n",
        "        D_real = D(real_images)\n",
        "        d_real_loss = real_loss(D_real, smooth=True)\n",
        "        \n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "        z = torch.from_numpy(z).float()\n",
        "        fake_images = G(z)\n",
        "              \n",
        "        D_fake = D(fake_images)\n",
        "        d_fake_loss = fake_loss(D_fake)\n",
        "        \n",
        "\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        g_optimizer.zero_grad()\n",
        "        \n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "        z = torch.from_numpy(z).float()\n",
        "        fake_images = G(z)\n",
        "        \n",
        "\n",
        "        D_fake = D(fake_images)\n",
        "        g_loss = real_loss(D_fake) # use real loss to flip labels\n",
        "        \n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        if batch_i % print_every == 0:\n",
        "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
        "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "    \n",
        "\n",
        "    losses.append((d_loss.item(), g_loss.item()))\n",
        "    \n",
        "    G.eval() # eval mode for generating samples\n",
        "    samples_z = G(fixed_z)\n",
        "    samples.append(samples_z)\n",
        "    G.train() # back to train mode\n",
        "\n",
        "\n",
        "with open('train_samples.pkl', 'wb') as f:\n",
        "    pkl.dump(samples, f)"
      ],
      "metadata": {
        "id": "9wTtqnwa0Kef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Résultats de l'entraînement"
      ],
      "metadata": {
        "id": "I6beHdLD1QU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "losses = np.array(losses)\n",
        "plt.plot(losses.T[0], label='Discriminator')\n",
        "plt.plot(losses.T[1], label='Generator')\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "GNdfzV-K0gbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Affichage d'images générées"
      ],
      "metadata": {
        "id": "aT1fm3JM1UBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_samples(epoch, samples):\n",
        "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
        "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
        "        img = img.detach()\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')"
      ],
      "metadata": {
        "id": "DNcxUfFh0ikX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_samples.pkl', 'rb') as f:\n",
        "    samples = pkl.load(f)"
      ],
      "metadata": {
        "id": "FEDokRam0lw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_samples(-1, samples)"
      ],
      "metadata": {
        "id": "nrAbkHo702je"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}